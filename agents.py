from typing import Dict, List, Any, TypedDict, Union, Optional
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage
from langchain_core.prompts import ChatPromptTemplate
import operator
import json
from dotenv import load_dotenv
import os
import regex as re
from datetime import datetime

# Load environment variables from .env file
load_dotenv()

# Constants
INTERVIEW_PROMPT = """# Role: Intellegent Adaptive Meeting Assistant
Goal: Gather structured information for meeting minutes while avoiding redundancy.

## Workflow
1. Pre-Process Notes:
   - Extract answers from user-provided notes upfront (e.g., "Company: XYZ Corp, Challenges: Scaling" → auto-populate fields).
   - Skip questions already answered.

2. Core Questions (Ask Only If Missing):
   - "What is the company name?"
   - "Who attended the meeting?"
   - "Where/long did it take place?"
   - "Employees/management levels?"

3. Dynamic Exploration:
   - Ask ONE question at a time from these topics only if unanswered:
     - Strategic goals → "What's the #1 priority for Q3?"
     - Development focus → "Which initiatives need acceleration?"
     - Challenges → "What's the biggest roadblock?"
     - Action items → "Who owns [task] and by when?"
     - Follow-up timing → "When should we review progress?"

4. Adaptive Rules:
   - Before asking ANY question:
     - Check conversation history for answers.
     - If answer exists:
       - ✔️ Confirm: "You mentioned [X]. Is this correct?"
       - ➡️ Clarify if ambiguous: "For [X], did you mean [interpretation]?"
     - If incomplete → ask follow-ups: "Can you elaborate on [specific detail]?"

5. Format:
   - Conversational but professional.
   - Always summarize key points before moving to next topic.

Note: Always cross-check that all needed questions are answered. If all questions have been answered, inform the user that all questions are done."""

# System prompts for Intelligent and Reflection Agents
INTELLIGENT_AGENT_SYSTEM_PROMPT = """You are an Intelligent Agent in a meeting assistant system for a business consultancy company. Your task is to generate insightful questions based on meeting notes and conversation history to support consultants in improving client performance. Create a mix of open-ended and specific questions that:

1. *Uncover gaps*: Identify missing details critical to strategic or operational decisions.
2. *Test assumptions*: Probe the logic behind plans or statements to ensure they’re robust.
3. *Spark innovation*: Suggest new angles or approaches to enhance business outcomes.
4. *Drive action*: Explore consequences, risks, or next steps tied to key points.

Target these consultancy-relevant themes:
- *Strategic goals and plans*: Alignment with business vision and market trends.
- *Management and leadership needs*: Skill gaps or development opportunities.
- *Business challenges*: Operational, financial, or competitive issues.
- *Action items*: Responsibilities and timelines for execution.

Ensure questions are:
- *Clear*: Simple and precise.
- *Relevant*: Linked to the meeting and client priorities.
- *Impactful*: Likely to yield strategic, actionable insights.

Use the meeting notes as your foundation, building on what’s discussed. Maintain a professional, engaging tone suited for business consultants.
"""

REFLECTION_SYSTEM_PROMPT = """You are a Reflection Agent in a meeting assistant system for a business consultancy company. Your role is to review and improve questions generated by the Intelligent Agent, ensuring they deliver value to consultants. For each question:

1. *Assess quality*:
   - *Clarity*: Is it easy to understand?
   - *Relevance*: Does it fit the meeting and consultancy goals?
   - *Depth*: Will it lead to meaningful, strategic insights?
   - *Flaws*: Any ambiguity, bias, or overcomplexity?

2. *Give feedback*:
   - Point out issues (e.g., “Too vague—needs a specific focus”).
   - Suggest fixes (e.g., “Add a metric like ROI to sharpen it”).

3. *Refine it*:
   - Rewrite the question to fix flaws, keeping it consultancy-focused.

4. *Check the set*:
   - *Coverage*: Are all key areas (strategy, operations, actions) addressed?
   - *Uniqueness*: No overlap between questions?
   - *Flow*: Do they work together logically?

Focus on creating questions that drive actionable, performance-focused insights. Use a critical, constructive tone, suitable for a professional consultancy setting.
"""

# System prompt for Final MoM Agent
FINAL_MOM_AGENT_SYSTEM_PROMPT = """You are an expert Meeting Minutes (MoM) generator for Atria, a consultancy firm specializing in strategic planning and training solutions. Your task is to create a professional, actionable MoM document based on the provided transcript, conversation history, and structured Q&A data. The MoM should be ready for immediate use by consultants for follow-ups and proposals.

---

## 📄 Meeting Minutes (MoM)

### 1. Meeting Overview
- **Client/Company Name:** [Extracted or 'Not Specified']
- **Meeting Date & Time:** [Extracted or 'Not Specified']
- **Location:** [Extracted or 'Virtual']
- **Duration:** [If known, e.g., 'Approx. 30 mins']
- **Participants:** [List names and roles if mentioned, e.g., 'Margarita (Client), Dan (Atria Consultant)']

### 2. Objectives of the Meeting
- Summarize the purpose of the meeting in one sentence (e.g., 'Discuss sales training needs for client advisors and explore collaboration with Atria').

### 3. Key Discussion Points
- **Strategic Goals:** [List any business or strategic goals discussed, e.g., 'Shift to customer-centric product development']
- **Challenges:** [Identify pain points or hurdles, e.g., 'Lack of regular sales training']
- **Training & Development Needs:** [Detail target audience, skills, formats, e.g., 'Sales training for ~300 client advisors, face-to-face']
- **Other Themes:** [Capture additional insights, e.g., 'Evaluation process for training proposals']

### 4. Agreed Action Items
- Use bullet points. For each item, include:
  - **Task Description** (e.g., 'Draft training proposal for both options')
  - **Responsible Person** (if specified, e.g., 'Dan') or 'TBD'
  - **Deadline or Timeline** (if specified, e.g., 'End of next week') or 'Pending Confirmation'

### 5. Follow-Up & Next Steps
- **Scheduled Follow-Up:** [If discussed, e.g., 'Review proposal in 2 weeks']
- **Pending Information:** [List any open items or clarifications needed, e.g., 'Clarify training duration preference']

### 6. Consultant Notes (Optional)
- Add insights for Atria's internal use (e.g., 'Client prioritizes face-to-face training; consider budget flexibility').

---

### 🔧 Instructions
- Extract information **only** from the provided data (transcript, Q&A, history). Do not fabricate details.
- Use 'Not Specified' or 'TBD' for missing information.
- Maintain a concise, professional tone with clear headings and bullet points.
- Ensure the MoM is actionable and ready for use in proposals or task tracking.
- Highlight training-specific details relevant to Atria's services.

Generate the final MoM using the data provided."""

# Define integrated state structure
class IntegratedState(TypedDict):
    # Tool 1 and Q&A Agent States
    conversation_history: List[BaseMessage]
    meeting_details: Optional[Dict[str, Any]]
    current_question: Optional[str]
    interview_completed: bool
    answered_questions: List[Dict[str, str]]
    mode: str
    next_node: str
    status: str
    
    # Intelligent Agent States
    notes: Optional[str]
    intelligent_conversation_history: Optional[List[Dict]]
    initial_questions: Optional[List[str]]
    generated_questions: Optional[List[str]]
    reflection_feedback: Optional[str]
    iteration_count: Optional[int]
    max_iterations: Optional[int]
    
    # Classification Tool States
    classified_answered_questions: Optional[List[str]]
    classified_unanswered_questions: Optional[List[str]]
    
    # Question Answer Manager States
    qa_manager_answered_questions: Optional[List[Dict[str, str]]]
    qa_manager_complete: bool
    
    # Final MoM Agent States
    final_mom_content: Optional[str]
    mom_generation_complete: Optional[bool]
    mom_generated_timestamp: Optional[str]
    error_message: Optional[str]
    
    # Workflow Control
    current_phase: str
    workflow_complete: bool

# Helper functions
def get_openai_model():
    return ChatOpenAI(model="gpt-4o-mini")

def initialize_state() -> IntegratedState:
    return {
        # Tool 1 and Q&A Agent States
        "conversation_history": [],
        "meeting_details": {},
        "current_question": "",
        "interview_completed": False,
        "answered_questions": [],
        "mode": "",
        "next_node": "",
        "status": "",
        
        # Intelligent Agent States
        "notes": "",
        "intelligent_conversation_history": [],
        "initial_questions": [],
        "generated_questions": [],
        "reflection_feedback": "",
        "iteration_count": 0,
        "max_iterations": 2,
        
        # Classification Tool States
        "classified_answered_questions": [],
        "classified_unanswered_questions": [],
        
        # Question Answer Manager States
        "qa_manager_answered_questions": [],
        "qa_manager_complete": False,
        
        # Final MoM Agent States
        "final_mom_content": "",
        "mom_generation_complete": False,
        "mom_generated_timestamp": "",
        "error_message": "",
        
        # Workflow Control
        "current_phase": "initial",
        "workflow_complete": False
    }

# PHASE 2: Q&A Agent Node
def qa_agent_node(state: IntegratedState) -> IntegratedState:
    updated_state = state.copy()
    
    # Q&A mode
    llm = get_openai_model()
    
    # Create prompt template with the interview strategy
    interview_prompt_template = ChatPromptTemplate.from_messages([
        ("system", INTERVIEW_PROMPT),
        ("human", "{current_context}")
    ])
    
    # Initial interaction or continuing the interview
    if not updated_state.get("conversation_history"):
        # Generate the first question using the system prompt
        initial_context = "Begin the meeting information gathering process. Generate the first question to start collecting meeting details."
        
        first_question_response = llm.invoke(
            interview_prompt_template.format_messages(current_context=initial_context)
        )
        
        first_question = first_question_response.content
        
        updated_state["conversation_history"] = [AIMessage(content=first_question)]
        updated_state["current_question"] = first_question
        updated_state["meeting_details"] = {}
        updated_state["interview_completed"] = False
        updated_state["answered_questions"] = []
        updated_state["current_phase"] = "intelligent_agent_phase"  # Set next phase directly
    
    return updated_state

def process_user_input(state: IntegratedState, user_input: str) -> IntegratedState:
    """
    Process user input and update the state
    """
    # Add user input to conversation history
    updated_state = state.copy()
    updated_state["conversation_history"].append(HumanMessage(content=user_input))
    
    # Update answered questions
    if updated_state["current_question"]:
        updated_state["answered_questions"].append({
            "question": updated_state["current_question"],
            "answer": user_input
        })
    
    # Get LLM
    llm = get_openai_model()
    
    # Create prompt template
    interview_prompt_template = ChatPromptTemplate.from_messages([
        ("system", INTERVIEW_PROMPT),
        ("human", "{current_context}")
    ])
    
    # Prepare context for next question generation
    context = f"""
    Current Meeting Details: {updated_state.get('meeting_details', {})}
    Previous Answers: {updated_state.get('answered_questions', [])}
    Last User Response: {user_input}
    
    Based on the interview strategy, generate the next most appropriate question to continue gathering meeting information.
    """
    
    # Generate next question or process completion
    next_question_response = llm.invoke(
        interview_prompt_template.format_messages(current_context=context)
    )
    
    next_question = next_question_response.content
    
    # Check if interview is complete
    interview_completed = "all questions are done" in next_question.lower()
    
    # Update state with new information
    updated_state["conversation_history"].append(AIMessage(content=next_question))
    updated_state["current_question"] = next_question
    updated_state["interview_completed"] = interview_completed
    
    return updated_state

def prepare_notes_from_qa(state: IntegratedState) -> str:
    """Convert Q&A information to structured notes for Intelligent Agent"""
    notes = "Meeting Notes:\n"
    
    # Add meeting details
    if state.get("meeting_details"):
        notes += "\nMeeting Details:\n"
        for key, value in state["meeting_details"].items():
            notes += f"- {key}: {value}\n"
    
    # Add question-answer pairs
    if state.get("answered_questions"):
        notes += "\nDiscussion Points:\n"
        for qa in state["answered_questions"]:
            notes += f"Q: {qa.get('question')}\n"
            notes += f"A: {qa.get('answer')}\n\n"
    
    return notes

def convert_messages_to_dict(messages: List[BaseMessage]) -> List[Dict]:
    """Convert BaseMessage objects to dictionaries for Intelligent Agent"""
    result = []
    for msg in messages:
        if isinstance(msg, HumanMessage):
            result.append({"role": "user", "content": msg.content})
        elif isinstance(msg, AIMessage):
            result.append({"role": "assistant", "content": msg.content})
        elif isinstance(msg, SystemMessage):
            result.append({"role": "system", "content": msg.content})
    return result

# PHASE 3: Intelligent Agent Functions
def intelligent_agent_node(state: IntegratedState) -> IntegratedState:
    """Generate intelligent questions based on meeting notes and conversation history"""
    updated_state = state.copy()
    
    # Initialize intelligent_conversation_history if None or convert to list if needed
    if updated_state.get("intelligent_conversation_history") is None:
        updated_state["intelligent_conversation_history"] = []
    
    # Format conversation history, with additional error handling
    formatted_history = ""
    history_items = updated_state.get("intelligent_conversation_history", [])
    
    # Ensure history_items is a list
    if not isinstance(history_items, list):
        history_items = []
    
    for message in history_items:
        # Handle both dictionary and object formats safely
        if isinstance(message, dict):
            role = message.get("role", "")
            content = message.get("content", "")
        else:
            role = getattr(message, "role", "")
            content = getattr(message, "content", "")
            
        formatted_history += f"{role}: {content}\n\n"

    # Enhanced prompt to handle structured notes better
    prompt_template = ChatPromptTemplate.from_messages([
        ("system", INTELLIGENT_AGENT_SYSTEM_PROMPT),
        ("human", f"""
        Notes from the meeting:
        {{notes}}
        Extracted Meeting Information:
        {{formatted_history}}
        Based on the above meeting notes and extracted information, generate insightful 
        questions that address information gaps or areas that need further clarification. 
        Focus particularly on:
        1. Strategic goals and their implementation
        2. Management development needs
        3. Current challenges and their potential solutions
        4. Action items and follow-up processes
        Please provide 5-8 well-formulated questions.
        """)
    ])


    # Format the prompt with actual data
    formatted_prompt = prompt_template.format_messages(
        notes=updated_state["notes"],
        formatted_history=formatted_history
    )

    # Generate questions using the formatted prompt
    llm = get_openai_model()
    response = llm.invoke(formatted_prompt)

    # Parse questions from response and clean up formatting
    try:
        # Try to parse as JSON if formatted that way
        questions = json.loads(response.content)
    except:
        # Split by newlines and clean up numbering
        questions = []
        for line in response.content.split('\n'):
            # Remove common number patterns and clean up
            cleaned_line = line.strip()
            # Remove leading numbers and dots (e.g., "1.", "1)", "[1]", etc.)
            cleaned_line = re.sub(r'^\d+[\.\)\]]\s*', '', cleaned_line)
            if cleaned_line:
                questions.append(cleaned_line)
        
        # Filter out empty lines and lines that are just numbers
        questions = [q for q in questions if q and not q.isdigit()]

    # Update state with cleaned questions
    if updated_state["iteration_count"] == 0:
        updated_state["initial_questions"] = questions
    
    updated_state["generated_questions"] = questions
    updated_state["iteration_count"] += 1
    updated_state["current_phase"] = "reflection_agent_phase"
    
    return updated_state

# PHASE 4: Reflection Agent Function
def reflection_agent_node(state: IntegratedState) -> IntegratedState:
    """Reflection agent that critiques questions from the Intelligent Agent."""
    updated_state = state.copy()
    
    # Extract the current questions
    questions = updated_state["generated_questions"]
    questions_text = "\n".join([f"{i+1}. {q}" for i, q in enumerate(questions)])
    
    # Create the prompt for the reflection agent
    prompt = f"""Here are the questions generated by the Intelligent Agent:
{questions_text}

Please critique these questions and provide feedback on how they can be improved.
After providing feedback, please provide an improved version of each question.

Format your response with:
FEEDBACK: [Your detailed critique and suggestions]

IMPROVED QUESTIONS:
1. [First improved question]
2. [Second improved question]
...and so on
"""
    
    # Get feedback from the LLM
    llm = get_openai_model()
    messages = [
        SystemMessage(content=REFLECTION_SYSTEM_PROMPT),
        HumanMessage(content=prompt)
    ]
    
    response = llm.invoke(messages)
    content = response.content
    
    # Extract feedback and improved questions
    feedback = ""
    improved_questions = []
    
    if "FEEDBACK:" in content and "IMPROVED QUESTIONS:" in content:
        parts = content.split("IMPROVED QUESTIONS:")
        feedback = parts[0].replace("FEEDBACK:", "").strip()
        
        questions_part = parts[1].strip()
        question_lines = [q.strip() for q in questions_part.split('\n') if q.strip()]
        improved_questions = [q.lstrip("0123456789. ") for q in question_lines if q]
    else:
        feedback = content
    
    # Update state with reflection results
    updated_state["reflection_feedback"] = feedback
    if improved_questions:
        updated_state["generated_questions"] = improved_questions
    
    updated_state["current_phase"] = "classification_tool_phase"
    return updated_state

# PHASE 5: Classification Tool Functions
def extract_keywords(question):
    """Extract key terms from a question for matching purposes."""
    # Basic implementation - remove common words and punctuation
    common_words = {"what", "when", "where", "who", "why", "how", "is", "are", "was", "were", 
                   "do", "does", "did", "the", "a", "an", "in", "on", "at", "to", "for"}
    
    # Convert to lowercase and remove punctuation
    question_lower = ''.join(c for c in question.lower() if c.isalnum() or c.isspace())
    
    # Split into words and filter out common words
    keywords = [word for word in question_lower.split() if word not in common_words]
    
    return keywords

def has_been_answered(question, keywords, history_text, llm=None):
    """
    Determine if a question has been answered in the conversation history.
    
    This can use simple keyword matching or leverage LLM for more complex cases.
    """
    # Simple approach: check if most keywords appear in history text
    keyword_match_threshold = 0.7  # 70% of keywords should match
    
    # Count how many keywords appear in the history
    matches = sum(1 for keyword in keywords if keyword in history_text.lower())
    match_ratio = matches / len(keywords) if keywords else 0
    
    if match_ratio >= keyword_match_threshold:
        return True
        
    # For more complex questions or ambiguous results, use LLM if available
    if llm and (0.3 <= match_ratio < keyword_match_threshold):
        prompt = f"""
        Based on this conversation history:
        ---
        {history_text[:2000]}  # Limit length for token constraints
        ---
        
        Has this question been answered? "{question}"
        Respond with ANSWERED or UNANSWERED only.
        """
        
        messages = [HumanMessage(content=prompt)]
        response = llm.invoke(messages)
        return "ANSWERED" in response.content.upper()
        
    return False

def classify_questions(refined_questions, conversation_history, llm=None):
    """
    Classifies questions as answered or unanswered based on conversation history.
    
    Args:
        refined_questions (list): List of questions generated by Reflection agent
        conversation_history (list): Past conversation messages/history
        llm: Optional LLM client for more complex matching
        
    Returns:
        tuple: (unanswered_questions, answered_questions)
    """
    unanswered_questions = []
    answered_questions = []
    
    # Convert conversation history to a single searchable text
    history_text = " ".join([msg.content if hasattr(msg, "content") else str(msg) for msg in conversation_history])
    
    for question in refined_questions:
        # Simple matching approach - check if the question or its key components
        # appear in responses within the conversation history
        question_keywords = extract_keywords(question)
        
        # Check if we can find evidence the question was answered
        if has_been_answered(question, question_keywords, history_text, llm):
            answered_questions.append(question)
        else:
            unanswered_questions.append(question)
            
    return unanswered_questions, answered_questions

def classification_tool_node(state: IntegratedState) -> IntegratedState:
    """Classification tool that categorizes questions as answered or unanswered"""
    updated_state = state.copy()
    
    # Get the refined questions from reflection agent
    refined_questions = updated_state["generated_questions"]
    conversation_history = updated_state["conversation_history"]
    llm = get_openai_model()
    
    # Classify questions
    unanswered_questions, answered_questions = classify_questions(
        refined_questions, 
        conversation_history,
        llm
    )
    
    # Update state with classification results
    updated_state["classified_unanswered_questions"] = unanswered_questions
    updated_state["classified_answered_questions"] = answered_questions
    
    # Set next phase
    if updated_state["iteration_count"] >= updated_state["max_iterations"]:
        if unanswered_questions:
            updated_state["current_phase"] = "question_answer_manager_phase"
        else:
            updated_state["workflow_complete"] = True
            updated_state["current_phase"] = "final_output"
    else:
        updated_state["current_phase"] = "intelligent_agent_phase"
    
    return updated_state

# PHASE 6: Question Answer Manager Node
def question_answer_manager_node(state: IntegratedState) -> IntegratedState:
    """
    Manages asking unanswered questions and collecting answers
    """
    updated_state = state.copy()
    
    # Check if already completed
    if updated_state.get("qa_manager_complete", False):
        updated_state["current_phase"] = "final_output"
        return updated_state
    
    # Get the unanswered questions from classification tool
    not_answered_questions = updated_state.get("classified_unanswered_questions", [])
    
    # Convert classified answered questions to proper format
    # We need to convert from list of strings to list of dicts with question/answer pairs
    already_answered = []
    for question in updated_state.get("classified_answered_questions", []):
        # Find the answer from conversation history
        answer = find_answer_in_history(question, updated_state["conversation_history"])
        already_answered.append({"question": question, "answer": answer or "Not explicitly answered"})
    
    # Initialize our working list to store new Q&A pairs
    newly_answered = []
    
    # Ask each unanswered question
    print("\n--- Asking Unanswered Questions ---")
    for question in not_answered_questions:
        print(f"Question: {question}")
        answer = input("Your answer: ")
        
        # Store the Q&A pair
        newly_answered.append({"question": question, "answer": answer})
        
        # Display progress
        remaining = len(not_answered_questions) - len(newly_answered)
        if remaining > 0:
            print(f"{remaining} questions remaining.\n")
    
    # Merge the lists (previously answered + newly answered)
    final_qa_list = already_answered + newly_answered
    
    # Display the complete Q&A list
    print("\n--- Complete Questions & Answers ---")
    for i, qa in enumerate(final_qa_list, 1):
        print(f"{i}. Question: {qa['question']}")
        print(f"   Answer: {qa['answer']}")
        print()
    
    # Update state
    updated_state["qa_manager_answered_questions"] = final_qa_list
    updated_state["qa_manager_complete"] = True
    updated_state["current_phase"] = "final_output"
    
    return updated_state

def find_answer_in_history(question, conversation_history):
    """
    Attempt to find an answer to a question in the conversation history
    """
    # This is a simplified implementation - in a real system you would want
    # to use a more sophisticated approach with NLP or LLM
    question_lower = question.lower()
    
    # Look for question in conversation
    for i, message in enumerate(conversation_history):
        if isinstance(message, AIMessage) and question_lower in message.content.lower():
            # Found the question, try to get the answer (next user message)
            if i+1 < len(conversation_history) and isinstance(conversation_history[i+1], HumanMessage):
                return conversation_history[i+1].content
    
    return None

# PHASE 7: Final MoM Agent Node
def final_mom_agent_node(state: IntegratedState) -> IntegratedState:
    """
    Final MoM Agent that generates comprehensive meeting minutes
    This agent synthesizes all available data to create professional meeting minutes
    """
    updated_state = state.copy()
    
    try:
        # Collect all available data
        meeting_notes = updated_state.get("notes", "")
        conversation_history = updated_state.get("conversation_history", [])
        qa_answered_questions = updated_state.get("qa_manager_answered_questions", [])
        meeting_details = updated_state.get("meeting_details", {})
        
        # Prepare comprehensive input for MoM generation
        mom_input = prepare_mom_input_data(
            meeting_notes, 
            conversation_history, 
            qa_answered_questions, 
            meeting_details
        )
        
        # Generate MoM using LLM
        llm = get_openai_model()
        
        mom_prompt = ChatPromptTemplate.from_messages([
            ("system", FINAL_MOM_AGENT_SYSTEM_PROMPT),
            ("human", """Please generate comprehensive meeting minutes based on the following information:

{mom_input}

Ensure the minutes follow the structured format outlined in the system prompt and maintain professional standards suitable for business stakeholders.""")
        ])
        
        # Generate the final MoM
        formatted_prompt = mom_prompt.format_messages(mom_input=mom_input)
        response = llm.invoke(formatted_prompt)
        
        # Store the generated MoM in state
        updated_state["final_mom_content"] = response.content
        updated_state["mom_generation_complete"] = True
        updated_state["mom_generated_timestamp"] = datetime.now().isoformat()
        updated_state["workflow_complete"] = True
        updated_state["current_phase"] = "complete"
        
        # Debug: Print to verify content is generated
        print(f"DEBUG: MoM generated successfully. Length: {len(response.content)} characters")
        print(f"DEBUG: First 100 characters: {response.content[:100]}...")
        
        return updated_state
        
    except Exception as e:
        print(f"DEBUG: Error in final_mom_agent_node: {str(e)}")
        updated_state["error_message"] = f"Final MoM Agent error: {str(e)}"
        return updated_state

def prepare_mom_input_data(meeting_notes, conversation_history, qa_answered_questions, meeting_details):
    """
    Prepare and structure all input data for MoM generation
    """
    input_text = ""
    
    # Add meeting details
    if meeting_details:
        input_text += "## Meeting Metadata:\n"
        for key, value in meeting_details.items():
            input_text += f"- {key}: {value}\n"
        input_text += "\n"
    
    # Add original meeting notes/transcript
    if meeting_notes:
        input_text += "## Original Meeting Content:\n"
        input_text += meeting_notes + "\n\n"
    
    # Add conversation history from Q&A phase
    if conversation_history:
        input_text += "## Q&A Session History:\n"
        for msg in conversation_history:
            if hasattr(msg, 'content'):
                role = "Assistant" if hasattr(msg, '__class__') and "AI" in str(msg.__class__) else "Participant"
                input_text += f"**{role}:** {msg.content}\n"
        input_text += "\n"
    
    # Add refined questions and answers
    if qa_answered_questions:
        input_text += "## Additional Information Gathered:\n"
        for i, qa in enumerate(qa_answered_questions, 1):
            question = qa.get("question", "")
            answer = qa.get("answer", "")
            input_text += f"**Q{i}:** {question}\n"
            input_text += f"**A{i}:** {answer}\n\n"
    
    return input_text

# Update the existing final_output_node to use the new Final MoM Agent
def final_output_node(state: IntegratedState) -> IntegratedState:
    """Generate a final summary of the workflow results using Final MoM Agent"""
    # Delegate to the Final MoM Agent
    return final_mom_agent_node(state)

# Routing function to determine next phase
def determine_next_phase(state: IntegratedState) -> str:
    """Route to the appropriate next phase based on current phase"""
    if state["workflow_complete"]:
        return END
    
    return state["current_phase"]

# Create the Integrated LangGraph
def create_integrated_graph():
    """Create the integrated workflow connecting all components"""
    # Initialize the graph
    workflow = StateGraph(IntegratedState)
    
    # Add nodes (removed "initial" node)
    workflow.add_node("qa_phase", qa_agent_node)
    workflow.add_node("intelligent_agent_phase", intelligent_agent_node)
    workflow.add_node("reflection_agent_phase", reflection_agent_node)
    workflow.add_node("classification_tool_phase", classification_tool_node)
    workflow.add_node("question_answer_manager_phase", question_answer_manager_node)
    workflow.add_node("final_output", final_output_node)
    
    # Add conditional edges for routing between phases (removed initial routing)
    workflow.add_conditional_edges(
        "qa_phase",
        determine_next_phase,
        {
            "qa_phase": "qa_phase",
            "intelligent_agent_phase": "intelligent_agent_phase",
            "reflection_agent_phase": "reflection_agent_phase",
            "classification_tool_phase": "classification_tool_phase",
            "question_answer_manager_phase": "question_answer_manager_phase",
            "final_output": "final_output",
            END: END
        }
    )

    workflow.add_conditional_edges(
        "intelligent_agent_phase",
        determine_next_phase,
        {
            "qa_phase": "qa_phase",
            "intelligent_agent_phase": "intelligent_agent_phase",
            "reflection_agent_phase": "reflection_agent_phase",
            "classification_tool_phase": "classification_tool_phase",
            "question_answer_manager_phase": "question_answer_manager_phase",
            "final_output": "final_output",
            END: END
        }
    )

    workflow.add_conditional_edges(
        "reflection_agent_phase",
        determine_next_phase,
        {
            "qa_phase": "qa_phase",
            "intelligent_agent_phase": "intelligent_agent_phase",
            "reflection_agent_phase": "reflection_agent_phase",
            "classification_tool_phase": "classification_tool_phase",
            "question_answer_manager_phase": "question_answer_manager_phase",
            "final_output": "final_output",
            END: END
        }
    )
    
    workflow.add_conditional_edges(
        "classification_tool_phase",
        determine_next_phase,
        {
            "qa_phase": "qa_phase",
            "intelligent_agent_phase": "intelligent_agent_phase",
            "reflection_agent_phase": "reflection_agent_phase",
            "classification_tool_phase": "classification_tool_phase",
            "question_answer_manager_phase": "question_answer_manager_phase",
            "final_output": "final_output",
            END: END
        }
    )

    workflow.add_conditional_edges(
        "question_answer_manager_phase",
        determine_next_phase,
        {
            "qa_phase": "qa_phase",
            "intelligent_agent_phase": "intelligent_agent_phase",
            "reflection_agent_phase": "reflection_agent_phase",
            "classification_tool_phase": "classification_tool_phase",
            "question_answer_manager_phase": "question_answer_manager_phase",
            "final_output": "final_output",
            END: END
        }
    )

    workflow.add_edge("final_output", END)

    # set the entry point to start directly with qa_phase
    workflow.set_entry_point("qa_phase")

    # compile the graph
    app = workflow.compile()

    return app

# Run the integrated workflow
def run_integrated_workflow():
    app = create_integrated_graph()
    
    # Initialize state
    state = initialize_state()
    # Set mode to meeting_notes by default since we're removing user choice
    state["mode"] = "meeting_notes"
    state["current_phase"] = "qa_phase"
    
    # Run the graph
    result = app.invoke(state)
    
    return result

if __name__ == "__main__":
    run_integrated_workflow()